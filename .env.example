# Private Equity Automation System - Environment Configuration
# Copy this file to .env.local and update the values

# =============================================================================
# LOCAL AI CONFIGURATION
# =============================================================================

# Ollama Configuration (Local LLM Server)
OLLAMA_HOST=http://localhost:11434
DEFAULT_LLM_MODEL=llama3.1:8b

# Available models (install with: ollama pull <model>):
# - llama3.1:8b (recommended for speed)
# - llama3.1:70b (high quality, requires 32GB+ RAM)
# - mistral:7b (excellent for structured analysis)
# - mistral:instruct (optimized for instructions)
# - codellama:13b (specialized for calculations)

# ChromaDB Configuration (Vector Database)
CHROMADB_URL=http://localhost:8000

# MCP Server Configuration (Model Context Protocol)
MCP_PORT=8080

# =============================================================================
# OPTIONAL: BACKUP API CONFIGURATION
# =============================================================================

# OpenRouter API (fallback when local AI is unavailable)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Alternative AI Model (if not using local)
AI_MODEL=anthropic/claude-sonnet-4

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# SQLite Database Path (relative to project root)
DATABASE_PATH=./data/pe_system.db

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================

# Next.js Configuration
NODE_ENV=development
PORT=8000

# Logging Level (debug, info, warn, error)
LOG_LEVEL=info

# =============================================================================
# ADVANCED AI SETTINGS
# =============================================================================

# RAG Configuration
RAG_MAX_DOCS=5
RAG_CONTEXT_WINDOW=4000
RAG_SIMILARITY_THRESHOLD=0.3

# LLM Generation Settings
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
LLM_TOP_P=0.9
LLM_TOP_K=40

# Vector Store Settings
VECTOR_STORE_MAX_DOCUMENTS=10000
VECTOR_STORE_EMBEDDING_MODEL=all-MiniLM-L6-v2

# MCP Server Settings
MCP_MAX_CLIENTS=100
MCP_HEARTBEAT_INTERVAL=30000
MCP_CLEANUP_INTERVAL=60000

# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Memory Management
NODE_OPTIONS=--max-old-space-size=8192

# Cache Settings
ENABLE_CONTEXT_CACHING=true
CACHE_TIMEOUT=1800000

# Concurrent Processing
MAX_CONCURRENT_ANALYSES=3
MAX_CONCURRENT_SCRAPING=5

# =============================================================================
# SECURITY SETTINGS
# =============================================================================

# Enable/Disable Features
ENABLE_FILE_UPLOAD=true
ENABLE_EXTERNAL_SCRAPING=true
ENABLE_MCP_SERVER=true

# Rate Limiting
RATE_LIMIT_WINDOW=900000
RATE_LIMIT_MAX_REQUESTS=100

# CORS Settings
CORS_ORIGIN=http://localhost:8000
CORS_CREDENTIALS=true

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Debug Mode
DEBUG_MODE=false
VERBOSE_LOGGING=false

# Test Configuration
TEST_MODE=false
MOCK_AI_RESPONSES=false

# Development Tools
ENABLE_HOT_RELOAD=true
ENABLE_SOURCE_MAPS=true

# =============================================================================
# PRODUCTION SETTINGS (for deployment)
# =============================================================================

# Production Database
# DATABASE_URL=postgresql://user:password@localhost:5432/pe_system

# Production AI Services
# PRODUCTION_LLM_ENDPOINT=https://your-production-llm-service.com
# PRODUCTION_VECTOR_STORE=https://your-vector-store-service.com

# SSL Configuration
# SSL_CERT_PATH=/path/to/cert.pem
# SSL_KEY_PATH=/path/to/key.pem

# Monitoring
# SENTRY_DSN=your_sentry_dsn_here
# ANALYTICS_ID=your_analytics_id_here

# =============================================================================
# NOTES
# =============================================================================

# 1. For local development, only the LOCAL AI CONFIGURATION section is required
# 2. Make sure Ollama is running: ollama serve
# 3. Make sure ChromaDB is running: chroma run --host localhost --port 8000
# 4. Install required models: ollama pull llama3.1:8b
# 5. For production, consider using environment-specific configurations
# 6. Never commit .env.local to version control
